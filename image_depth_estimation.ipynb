{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVq-09XU187G"
      },
      "source": [
        "### Introduction\n",
        "This notebook addresses the problem of **depth estimation in comics images**, which presents unique challenges compared to natural image depth estimation. Comics often feature exaggerated perspectives, inconsistent object scaling, and non-realistic visual elements, making it difficult to infer accurate depth information. The goal of this project is to predict two types of depth values in comics images: **intra-depth** (depth within objects) and **inter-depth** (depth between objects).\n",
        "\n",
        "To tackle this problem, we implement a deep learning solution using a **Convolutional Neural Network (CNN)**. The network is designed to extract hierarchical features from the images and predict depth values. The notebook follows a structured workflow:\n",
        "1. **Data Loading and Preprocessing**: Custom dataset loading that handles comics images and their depth annotations.\n",
        "2. **Model Architecture**: Two model versions are explored, including a baseline model and an improved model, which incorporates batch normalization, additional convolutional layers, and dropout for better generalization.\n",
        "3. **Training and Evaluation**: The models are trained using PyTorch, and their performance is evaluated on validation and test sets using mean squared error (MSE) as the evaluation metric.\n",
        "4. **Inference**: The trained model is applied to unseen test data to generate depth predictions, which are saved for further analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** to import the dataset needed for the Colab notebook, you need to add a shortcut to the shared folder to your Drive\n",
        "\n",
        "- Click on the shared folder name\n",
        "- \"Add shortcut to Drive\"\n",
        "- \"My Drive\" \n",
        "\n",
        "You can also run the notebook directly by opening it from the shared folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXMvJnLsAQdA",
        "outputId": "44172960-b7da-4d50-da86-5f22f8302669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Device:  cpu\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pytz\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import glob\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "PROJECT_PATH = '/content/drive/MyDrive/Progetto_Deep_Learning/'      #mettere il proprio path al progetto\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3wcHJ10UWTD"
      },
      "source": [
        "# **Data processing**\n",
        "\n",
        "In this section, we define a custom dataset class, `DepthDataset`, which is essential for loading and preprocessing the comics images along with their corresponding depth annotations. This class is specifically designed to handle the unique structure of our dataset, where each image is associated with two types of depth values: **intra-depth** (depth within an object) and **inter-depth** (depth between objects).\n",
        "\n",
        "The class performs the following key tasks:\n",
        "1. **Loading Image Data**: The images are loaded from a specified directory, resized to a uniform size of 128x128 pixels, and normalized to a [0, 1] range.\n",
        "2. **Loading Annotations**: Depth annotations are read from a JSON file, which provides the intra-depth and inter-depth values for each image.\n",
        "3. **Preprocessing**: The class applies any specified transformations (e.g., augmentations or tensor conversions) and ensures that the images and depth values are properly formatted for input into a deep learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HOHMbWcUAodH"
      },
      "outputs": [],
      "source": [
        "# Dataset class for custom dataset loading\n",
        "class DepthDataset(Dataset):\n",
        "    def __init__(self, data_dir, annotations_file, img_size=(128, 128), transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        - data_dir (str): La directory dove si trovano le immagini.\n",
        "        - annotations_file (str): Il file JSON contenente le annotazioni.\n",
        "        - img_size (tuple): La dimensione a cui ridimensionare le immagini (default: (128, 128)).\n",
        "        - transform: Trasformazioni da applicare alle immagini.\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        self.transform = transform\n",
        "\n",
        "        with open(annotations_file, 'r') as f:\n",
        "            self.annotations = json.load(f)\n",
        "\n",
        "        self.image_paths = []\n",
        "        self.intradepth = []\n",
        "        self.interdepth = []\n",
        "\n",
        "        # Processa ogni immagine e le sue annotazioni\n",
        "        for img_info in self.annotations['images']:\n",
        "            img_id = img_info['id']\n",
        "            img_name = img_info['file_name']\n",
        "            img_path = os.path.join(data_dir, img_name)\n",
        "\n",
        "            # Verifica che l'immagine esista\n",
        "            if not os.path.exists(img_path):\n",
        "                print(f\"Warning: Image {img_name} not found.\")\n",
        "                continue\n",
        "\n",
        "            self.image_paths.append(img_path)\n",
        "\n",
        "            # Trova le annotazioni per questa immagine\n",
        "            for annotation in self.annotations['annotations']:\n",
        "                if annotation['image_id'] == img_id:\n",
        "                    self.intradepth.append(annotation['attributes'].get('Intradepth', 0))\n",
        "                    self.interdepth.append(annotation['attributes'].get('Interdepth', 0))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.resize(image, self.img_size)\n",
        "        image = image / 255.0  # Normalizza\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Converte l'immagine in torch.float32\n",
        "        image = image.float()\n",
        "\n",
        "        intradepth = torch.tensor(self.intradepth[idx], dtype=torch.float32)\n",
        "        interdepth = torch.tensor(self.interdepth[idx], dtype=torch.float32)\n",
        "\n",
        "        return image, intradepth, interdepth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UVLZwXXUdI4"
      },
      "source": [
        "# **Model architecture definition**\n",
        "In this section, we define two convolutional neural network (CNN) architectures for the task of depth estimation in comics images: the **default model** and an enhanced **Version 2 model**. These models are designed to predict two types of depth valuesâ€”**intra-depth** (depth within an object) and **inter-depth** (depth between objects).\n",
        "\n",
        "1. **Default Model**:\n",
        "   The default model is a basic CNN architecture that consists of two convolutional layers followed by max-pooling for downsampling, a fully connected layer, and two separate output layers for intra-depth and inter-depth predictions. While this model provides a solid starting point, its simplicity may limit its ability to generalize well on complex visual scenes in comics.\n",
        "\n",
        "."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2alcOzTiBDk6"
      },
      "outputs": [],
      "source": [
        "# Modello default\n",
        "class DepthOrderingModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DepthOrderingModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(128 * 128 * 128, 256)\n",
        "        self.fc_inter = nn.Linear(256, 1)\n",
        "        self.fc_intra = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.upsample(x)\n",
        "        x = self.flatten(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        inter_depth = self.fc_inter(x)\n",
        "        intra_depth = self.fc_intra(x)\n",
        "        return inter_depth, intra_depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abRcZeq0w892"
      },
      "source": [
        "2. **Version 2 Model**:\n",
        "   The Version 2 model introduces several improvements over the default model:\n",
        "   - **Additional Convolutional Layer**: A third convolutional layer is added to extract more complex features from the images.\n",
        "   - **Batch Normalization**: After each convolutional layer, batch normalization is applied to stabilize and accelerate training.\n",
        "   - **Global Average Pooling**: Replaces the flattening operation to reduce the number of parameters and improve generalization.\n",
        "   - **Dropout**: A 10% dropout layer is included before the fully connected layer to prevent overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2KoYKllwbH0o"
      },
      "outputs": [],
      "source": [
        "#modello v2\n",
        "class DepthOrderingModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DepthOrderingModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)  # Global average pooling\n",
        "        self.dropout = nn.Dropout(p=0.1)  # Dropout with 10% probability\n",
        "\n",
        "        self.fc1 = nn.Linear(256, 128)\n",
        "        self.fc_inter = nn.Linear(128, 1)\n",
        "        self.fc_intra = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = torch.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.global_avg_pool(x)  # Shape: (batch_size, 256, 1, 1)\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "\n",
        "        inter_depth = self.fc_inter(x)\n",
        "        intra_depth = self.fc_intra(x)\n",
        "        return inter_depth, intra_depth\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c-25WLjTpz-"
      },
      "source": [
        "# **Training**\n",
        "In this section, we define the complete training pipeline for the **Depth Ordering Model**, ensuring that the model can be trained, validated, and saved efficiently. This part of the notebook is responsible for managing the training process, data loading, and model saving in an organized and automated manner.\n",
        "\n",
        "Key steps in this cell include:\n",
        "\n",
        "1. **Data Preparation**:\n",
        "   - We define the transformations needed for the images (converting to tensors) and load the dataset from the project directory.\n",
        "   - The dataset is split into training and validation sets, following an 80/20 ratio.\n",
        "\n",
        "2. **Model Initialization**:\n",
        "   - The `DepthOrderingModel` is instantiated and transferred to the available device (GPU if available).\n",
        "   \n",
        "3. **Training Loop**:\n",
        "   - The model is trained over 30 epochs, with each epoch consisting of a forward pass, loss calculation, backpropagation, and optimizer step.\n",
        "   - The model is evaluated on the validation set after each epoch to monitor performance.\n",
        "   - The `ReduceLROnPlateau` scheduler dynamically adjusts the learning rate if the validation loss plateaus.\n",
        "\n",
        "4. **Early Stopping and Model Saving**:\n",
        "   - The script implements early stopping, terminating the training process if the validation loss does not improve for 5 consecutive epochs.\n",
        "   - The best performing model (in terms of validation loss) is saved automatically with a timestamp in the filename, ensuring that the optimal model is preserved without manual intervention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rRREpMIBOAB",
        "outputId": "9474732d-ba90-4012-ae35-dd3fc664a83c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30, Training Loss: 2.5653\n",
            "Validation Loss: 2.3268\n",
            "Validation Loss: 1.8595\n",
            "Epoch 3/30, Training Loss: 1.6458\n",
            "Validation Loss: 2.5933\n",
            "Epoch 4/30, Training Loss: 1.5324\n",
            "Validation Loss: 2.3803\n",
            "Epoch 5/30, Training Loss: 1.5065\n",
            "Validation Loss: 2.9026\n",
            "Epoch 6/30, Training Loss: 1.4849\n",
            "Validation Loss: 3.7843\n",
            "Epoch 7/30, Training Loss: 1.4303\n",
            "Validation Loss: 2.9217\n",
            "Early stopping triggered\n",
            "Training completed.\n",
            "Best validation loss: 1.8595452457666397\n"
          ]
        }
      ],
      "source": [
        "# Definisco le trasformazioni per le immagini\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Dati di training\n",
        "data_dir = PROJECT_PATH + 'data/images/train/'  #nella cartella del progetto\n",
        "annotations_file = PROJECT_PATH + 'data/annotations/train-annotations.json'\n",
        "saving_directory = PROJECT_PATH + 'models/best_model_'+str(datetime.now(pytz.timezone('Europe/Rome')).strftime(\"%Y-%m-%d_%H:%M\")) + '.pth'\n",
        "\n",
        "dataset = DepthDataset(data_dir, annotations_file, transform=transform)\n",
        "\n",
        "# Divido il dataset in training e validation set\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Istanzia il modello e lo sposta sulla GPU se disponibile\n",
        "model = DepthOrderingModel().to(device)\n",
        "\n",
        "# Initialize optimizer and learning rate scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "num_epochs = 30  # Increased epochs\n",
        "patience = 5\n",
        "best_val_loss = float('inf')\n",
        "early_stop_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for inputs, y_intra, y_inter in train_loader:\n",
        "        inputs, y_intra, y_inter = inputs.to(device), y_intra.to(device), y_inter.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs_inter, outputs_intra = model(inputs)\n",
        "        loss_inter = criterion(outputs_inter.squeeze(), y_inter)\n",
        "        loss_intra = criterion(outputs_intra.squeeze(), y_intra)\n",
        "\n",
        "        # Loss balancing (if needed, based on dataset scale)\n",
        "        loss = loss_inter + loss_intra\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, y_intra, y_inter in val_loader:\n",
        "            inputs, y_intra, y_inter = inputs.to(device), y_intra.to(device), y_inter.to(device)\n",
        "            outputs_inter, outputs_intra = model(inputs)\n",
        "            loss_inter = criterion(outputs_inter.squeeze(), y_inter)\n",
        "            loss_intra = criterion(outputs_intra.squeeze(), y_intra)\n",
        "            val_loss += (loss_inter.item() + loss_intra.item())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Early stopping logic\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), saving_directory)  # Save the best model\n",
        "        early_stop_counter = 0\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "\n",
        "    if early_stop_counter >= patience:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "\n",
        "print(\"Training completed.\")\n",
        "print(\"Best validation loss:\", best_val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ky9MzuIWUds"
      },
      "source": [
        "# **Model Evaluation**\n",
        "This section of the notebook performs the evaluation of the latest trained **Depth Ordering Model** on the validation dataset. It automates the process of loading the most recently trained model, applying it to the validation images, and calculating the evaluation metrics, specifically the **mean squared error (MSE)** for both **intra-depth** and **inter-depth** predictions.\n",
        "\n",
        "The evaluation process is structured as follows:\n",
        "\n",
        "1. **Loading the Latest Model**:\n",
        "   - The script automatically retrieves the most recent model from the saved models directory. This ensures that you are always evaluating the latest version without manually specifying the model path.\n",
        "\n",
        "2. **Ground Truth and Image Loading**:\n",
        "   - The ground truth annotations (intra-depth and inter-depth) are loaded from a JSON file.\n",
        "   - Validation images are preprocessed, including resizing, normalization, and conversion into PyTorch tensors, preparing them for input to the model.\n",
        "\n",
        "3. **Model Inference**:\n",
        "   - The validation images are passed through the model to generate predictions for both intra-depth and inter-depth.\n",
        "   - These predictions are flattened into 1D arrays to match the structure of the ground truth annotations.\n",
        "\n",
        "4. **Evaluation**:\n",
        "   - The predictions are compared against the ground truth annotations using the **mean squared error (MSE)** metric for both intra-depth and inter-depth. The overall MSE is also calculated by averaging these two values.\n",
        "   \n",
        "5. **Saving Results**:\n",
        "   - The evaluation metrics (MSE values) are saved to a text file in the results directory, along with the date and time of the evaluation and the model path.\n",
        "   - This ensures that each evaluation is logged and can be easily referenced in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoVov8FlTZsL",
        "outputId": "75998905-07b5-42fb-b0d5-ab3db4678b15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Image ID 91 or Category ID 19 not found in ground truth\n",
            "Warning: Image ID 134 or Category ID 18 not found in ground truth\n",
            "Warning: Image ID 135 or Category ID 20 not found in ground truth\n",
            "Warning: Image ID 183 or Category ID 24 not found in ground truth\n",
            "Warning: Image ID 248 or Category ID 19 not found in ground truth\n",
            "Warning: Image ID 259 or Category ID 24 not found in ground truth\n",
            "MSE of Inter-depth: 1.322116233084517\n",
            "MSE of Intra-depth: 2.4228754494645957\n",
            "Overall MSE: 1.8724958412745565\n",
            "CPU times: user 11.8 s, sys: 1.13 s, total: 12.9 s\n",
            "Wall time: 13.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# geth the last model trained\n",
        "directory = PROJECT_PATH + 'models/'\n",
        "list_of_files = sorted(filter(os.path.isfile, glob.glob(directory + '*')), key=os.path.getmtime)\n",
        "latest_file = list_of_files[-1]\n",
        "\n",
        "#paths\n",
        "model_path = latest_file  # Path al modello PyTorch\n",
        "val_data_dir = PROJECT_PATH + 'data/images/val/'              #Caricamento dei dati di validazione\n",
        "gt_path = PROJECT_PATH + 'data/annotations/val-annotations.json'  # Path agli annotations del ground truth\n",
        "results_dir = PROJECT_PATH + 'results'           #risultati della valutazione\n",
        "\n",
        "# Funzione per valutare il modello\n",
        "def evaluate_model(predictions, ground_truth):\n",
        "    pred_inter_depths = []\n",
        "    pred_intra_depths = []\n",
        "    true_inter_depths = []\n",
        "    true_intra_depths = []\n",
        "\n",
        "    # Processa ogni previsione\n",
        "    for index, row in predictions.iterrows():\n",
        "        img_id = str(row['img_id'])\n",
        "        category_id = str(row['category_id'])\n",
        "\n",
        "        pred_inter_depth = row['pred_Interdepth']\n",
        "        pred_intra_depth = row['pred_Intradepth']\n",
        "\n",
        "        # Verifica se l'immagine e la categoria esistono nel ground truth\n",
        "        if img_id in ground_truth and category_id in ground_truth[img_id]:\n",
        "            true_inter_depth = ground_truth[img_id][category_id]['Interdepth']\n",
        "            true_intra_depth = ground_truth[img_id][category_id]['Intradepth']\n",
        "\n",
        "            # Aggiungi previsioni e ground truth\n",
        "            pred_inter_depths.append(pred_inter_depth)\n",
        "            pred_intra_depths.append(pred_intra_depth)\n",
        "            true_inter_depths.append(true_inter_depth)\n",
        "            true_intra_depths.append(true_intra_depth)\n",
        "        else:\n",
        "            print(f\"Warning: Image ID {img_id} or Category ID {category_id} not found in ground truth\")\n",
        "\n",
        "    # Calcolo del MSE\n",
        "    if len(true_inter_depths) > 0 and len(pred_inter_depths) > 0:\n",
        "        mse_inter = mean_squared_error(true_inter_depths, pred_inter_depths)\n",
        "        mse_intra = mean_squared_error(true_intra_depths, pred_intra_depths)\n",
        "        mse_overall = (mse_inter + mse_intra) / 2\n",
        "    else:\n",
        "        mse_inter, mse_intra, mse_overall = None, None, None\n",
        "        print(\"No valid data for MSE calculation.\")\n",
        "\n",
        "    return mse_inter, mse_intra, mse_overall\n",
        "\n",
        "\n",
        "# Carica le annotazioni ground truth\n",
        "with open(gt_path, 'r') as f:\n",
        "    ground_truth_data = json.load(f)\n",
        "\n",
        "# Trasforma i dati del ground truth per un lookup piÃ¹ semplice\n",
        "ground_truth = {}\n",
        "for ann in ground_truth_data['annotations']:\n",
        "    img_id = str(ann['image_id'])\n",
        "    category_id = str(ann['category_id'])\n",
        "    intradepth = ann['attributes'].get('Intradepth', 0)\n",
        "    interdepth = ann['attributes'].get('Interdepth', 0)\n",
        "    if img_id not in ground_truth:\n",
        "        ground_truth[img_id] = {}\n",
        "    ground_truth[img_id][category_id] = {\n",
        "        'Intradepth': intradepth,\n",
        "        'Interdepth': interdepth\n",
        "    }\n",
        "\n",
        "# Definizione del dispositivo (CPU o GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Carica il modello PyTorch\n",
        "model = DepthOrderingModel()  # Usa la stessa architettura del modello usata in fase di training\n",
        "if torch.cuda.is_available():\n",
        "    model.load_state_dict(torch.load(model_path, weights_only= True)) # Carica lo stato del modello su gpu\n",
        "else:\n",
        "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu'), weights_only= True)) # Carica lo stato del modello e mappa i pesi sulla CPU se la GPU non Ã¨ disponibile\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# Trasformazione delle immagini in tensori PyTorch\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converte le immagini in tensor\n",
        "    transforms.Resize((128, 128)),  # Ridimensiona le immagini\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizza le immagini\n",
        "])\n",
        "\n",
        "# Carica e processa le immagini\n",
        "X_val = []\n",
        "img_ids = []\n",
        "for img_info in ground_truth_data['images']:\n",
        "    img_id = img_info['id']\n",
        "    img_name = img_info['file_name']\n",
        "    img_path = os.path.join(val_data_dir, img_name)\n",
        "    image = cv2.imread(img_path)\n",
        "    if image is not None:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Converti in RGB\n",
        "        image = transform(image).to(device)  # Applica la trasformazione\n",
        "        X_val.append(image)\n",
        "        img_ids.append(img_id)\n",
        "\n",
        "# Converti la lista di immagini in un batch di tensori\n",
        "X_val = torch.stack(X_val)\n",
        "\n",
        "# Ottieni le previsioni dal modello PyTorch\n",
        "with torch.no_grad():\n",
        "    pred_intra_depth, pred_inter_depth = model(X_val)\n",
        "\n",
        "# Trasforma le previsioni in array 1D\n",
        "pred_intra_depth = pred_intra_depth.cpu().numpy().flatten()\n",
        "pred_inter_depth = pred_inter_depth.cpu().numpy().flatten()\n",
        "\n",
        "# Assicurati che tutte le liste abbiano la stessa lunghezza\n",
        "category_ids = [cat_id for img in ground_truth.values() for cat_id in img.keys()]\n",
        "min_length = min(len(pred_intra_depth), len(pred_inter_depth), len(img_ids), len(category_ids))\n",
        "\n",
        "# Taglia gli array alla lunghezza minima\n",
        "pred_intra_depth = pred_intra_depth[:min_length]\n",
        "pred_inter_depth = pred_inter_depth[:min_length]\n",
        "img_ids = img_ids[:min_length]\n",
        "category_ids = category_ids[:min_length]\n",
        "\n",
        "# Crea il DataFrame delle previsioni\n",
        "predictions_df = pd.DataFrame({\n",
        "    'pred_Intradepth': pred_intra_depth,\n",
        "    'pred_Interdepth': pred_inter_depth,\n",
        "    'img_id': img_ids,\n",
        "    'category_id': category_ids\n",
        "})\n",
        "\n",
        "# Valuta il modello\n",
        "mse_inter, mse_intra, mse_overall = evaluate_model(predictions_df, ground_truth)\n",
        "\n",
        "\n",
        "if not os.path.exists(results_dir):\n",
        "    os.makedirs(results_dir)\n",
        "\n",
        "# Ottieni la data e l'ora corrente\n",
        "current_time = datetime.now(pytz.timezone('Europe/Rome')).strftime(\"%Y-%m-%d %H:%M\")\n",
        "\n",
        "# Scrivi i risultati nel file, includendo data e ora\n",
        "with open(os.path.join(results_dir, 'evaluation_metrics.txt'), 'a') as file:\n",
        "    file.write(f\"\\nEvaluation written on: {current_time}\\n\")\n",
        "    file.write(f'Model: {model_path}\\n')\n",
        "    file.write(f'MSE of Inter-depth: {mse_inter}\\n')\n",
        "    file.write(f'MSE of Intra-depth: {mse_intra}\\n')\n",
        "    file.write(f'Overall MSE: {mse_overall}\\n')\n",
        "\n",
        "print(f'MSE of Inter-depth: {mse_inter}')\n",
        "print(f'MSE of Intra-depth: {mse_intra}')\n",
        "print(f'Overall MSE: {mse_overall}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQOueZk1DJFL"
      },
      "source": [
        "# **Model Inference**\n",
        "This section of the notebook focuses on running **inference** using the latest trained **Depth Ordering Model** on a separate test dataset. The objective is to apply the trained model to unseen data, generate depth predictions, and store the results for further evaluation or analysis.\n",
        "\n",
        "Key steps in this cell include:\n",
        "\n",
        "1. **Loading the Latest Model**:\n",
        "   - The script automatically identifies and loads the most recently trained model from the saved models directory. This ensures consistency by always using the latest version without manual intervention.\n",
        "\n",
        "2. **Loading Test Annotations**:\n",
        "   - The ground truth depth annotations for the test dataset are loaded from a JSON file, which provides **intra-depth** and **inter-depth** values for each image category in the test set.\n",
        "   - A lookup dictionary is created to efficiently access the test segments data during prediction processing.\n",
        "\n",
        "3. **Data Preprocessing**:\n",
        "   - Test images are preprocessed through transformations such as resizing, normalization, and conversion into PyTorch tensors, making them suitable for input into the model.\n",
        "\n",
        "4. **Model Inference**:\n",
        "   - The test images are passed through the model to obtain predictions for intra-depth and inter-depth.\n",
        "   - The predicted depth values are flattened into 1D arrays.\n",
        "\n",
        "5. **Storing Results**:\n",
        "   - The predictions, along with their associated image and category IDs, are compiled into a Pandas DataFrame.\n",
        "   - The resulting DataFrame is then saved as a CSV file in the results directory for further evaluation or visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXR841zrDQr0",
        "outputId": "1d99318c-e0cc-4ec0-bf53-c9890c66cf8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    img_id category_id  pred_Intradepth  pred_Interdepth\n",
            "0      131          28         0.341507         0.700975\n",
            "1      158          22         0.223486         0.466395\n",
            "2      163           1         0.372284         0.699417\n",
            "3      188          16         0.344535         0.519607\n",
            "4      198          12         0.426177         0.770635\n",
            "5      207          25         0.392649         0.699364\n",
            "6      213          27         0.475495         0.848142\n",
            "7      245          24         0.337580         0.594709\n",
            "8      261          23         0.398065         0.787483\n",
            "9      269          11         0.316297         0.644748\n",
            "10     275          28         0.390858         0.697132\n",
            "11     282          12         0.418563         0.764017\n",
            "12       1          22         0.451510         0.864075\n",
            "13       8          23         0.729622         1.435773\n",
            "14      21          16         0.417270         0.823564\n",
            "15      25           1         0.271126         0.547276\n",
            "16      38          19         0.654990         1.304213\n",
            "17      62          13         0.796502         1.585559\n",
            "18      68          25         0.604128         1.144789\n",
            "19     101          24         0.445548         0.941498\n",
            "CPU times: user 6.04 s, sys: 279 ms, total: 6.32 s\n",
            "Wall time: 6.88 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# geth the last model trained\n",
        "directory = PROJECT_PATH + 'models/'\n",
        "list_of_files = sorted(filter(os.path.isfile, glob.glob(directory + '*')), key=os.path.getmtime)\n",
        "latest_file = list_of_files[-1]\n",
        "\n",
        "#paths\n",
        "model_path = latest_file  # Path dell'ultimo modello PyTorch\n",
        "test_data_dir = PROJECT_PATH + 'data/images/test/'              #Caricamento dei dati di validazione\n",
        "test_segments_path = PROJECT_PATH + 'data/annotations/depth_TEST_segments.json'\n",
        "results_dir = PROJECT_PATH + 'results'           #risultati della inferenza\n",
        "\n",
        "\n",
        "# Carica le annotazioni ground truth\n",
        "with open(test_segments_path, 'r') as f:\n",
        "    test_segments = json.load(f)\n",
        "\n",
        "# Trasforma i dati per un lookup piÃ¹ semplice\n",
        "test_segments_dict = {}\n",
        "for ann in test_segments['annotations']:\n",
        "    img_id = str(ann['image_id'])\n",
        "    category_id = str(ann['category_id'])\n",
        "    intradepth = ann['attributes'].get('Intradepth', 0)\n",
        "    interdepth = ann['attributes'].get('Interdepth', 0)\n",
        "    if img_id not in test_segments_dict:\n",
        "        test_segments_dict[img_id] = {}\n",
        "    test_segments_dict[img_id][category_id] = {\n",
        "        'Intradepth': intradepth,\n",
        "        'Interdepth': interdepth\n",
        "    }\n",
        "\n",
        "# Definizione del dispositivo (CPU o GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Carica il modello PyTorch\n",
        "model = DepthOrderingModel()  # Usa la stessa architettura del modello usata in fase di training\n",
        "if torch.cuda.is_available():\n",
        "    model.load_state_dict(torch.load(model_path, weights_only= True)) # Carica lo stato del modello su gpu\n",
        "else:\n",
        "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu'), weights_only= True)) # Carica lo stato del modello e mappa i pesi sulla CPU se la GPU non Ã¨ disponibile\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# Trasformazione delle immagini in tensori PyTorch\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converte le immagini in tensor\n",
        "    transforms.Resize((128, 128)),  # Ridimensiona le immagini\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizza le immagini\n",
        "])\n",
        "\n",
        "# Carica e processa le immagini\n",
        "X_test = []\n",
        "img_ids = []\n",
        "for img_info in test_segments['images']:\n",
        "    img_id = img_info['id']\n",
        "    img_name = img_info['file_name']\n",
        "    img_path = os.path.join(test_data_dir, img_name)\n",
        "    image = cv2.imread(img_path)\n",
        "    if image is not None:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Converti in RGB\n",
        "        image = transform(image).to(device)  # Applica la trasformazione\n",
        "        X_test.append(image)\n",
        "        img_ids.append(img_id)\n",
        "\n",
        "# Converti la lista di immagini in un batch di tensori\n",
        "X_test = torch.stack(X_test)\n",
        "\n",
        "# Ottieni le previsioni dal modello PyTorch\n",
        "with torch.no_grad():\n",
        "    pred_intra_depth, pred_inter_depth = model(X_test)\n",
        "\n",
        "# Trasforma le previsioni in array 1D\n",
        "pred_intra_depth = pred_intra_depth.cpu().numpy().flatten()\n",
        "pred_inter_depth = pred_inter_depth.cpu().numpy().flatten()\n",
        "\n",
        "# Assicurati che tutte le liste abbiano la stessa lunghezza\n",
        "category_ids = [cat_id for img in test_segments_dict.values() for cat_id in img.keys()]\n",
        "min_length = min(len(pred_intra_depth), len(pred_inter_depth), len(img_ids), len(category_ids))\n",
        "\n",
        "# Taglia gli array alla lunghezza minima\n",
        "pred_intra_depth = pred_intra_depth[:min_length]\n",
        "pred_inter_depth = pred_inter_depth[:min_length]\n",
        "img_ids = img_ids[:min_length]\n",
        "category_ids = category_ids[:min_length]\n",
        "\n",
        "# Crea il DataFrame delle previsioni\n",
        "predictions_df = pd.DataFrame({\n",
        "    'img_id': img_ids,\n",
        "    'category_id': category_ids,\n",
        "    'pred_Intradepth': pred_intra_depth,\n",
        "    'pred_Interdepth': pred_inter_depth\n",
        "})\n",
        "\n",
        "if not os.path.exists(results_dir):\n",
        "    os.makedirs(results_dir)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "pred_csv = predictions_df.to_csv(os.path.join(results_dir, 'test-predictions.csv'), index=False)\n",
        "print(predictions_df.to_string())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
